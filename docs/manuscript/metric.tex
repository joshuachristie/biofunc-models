\documentclass{article}
\usepackage{amsmath}
% \author{Joshua Christie, ...}

\title{Towards a quantitative measure of partial biological functions}
\begin{document}
\maketitle


The selected effects theory of function attempts to relate the evolution of traits to biological function. Couple problems: (i) aims for high level description, ignores a lot of biological reality, and so can't be applied to many realistic cases; (ii) treats notion of function as dichotomous, without quantitative treatment of partial functions. It's on this latter point that we will focus.

Expand upon how selected effects theory tries to have its cake and eat it too (they don't talk of partial functions but they clearly want to address the idea that a trait can have multiple functions...with the result being very vague)

Why do we need a theory of partial functions? Addresses a few problems: (i) accounts for a biological reality of traits having more than one function (examples); (ii) helps clarify examples where two different traits share underlying genes/pathways and so are not independent; (iii) allows us to compare different instances of evolution (e.g. one in which the causal part of natural selection is very high vs another in which it is low). So partial functions can operate on several axes.

So what would a quantiative theory of partial functions look like? Lay out a series of desirable characteristics.

Required characteristics:
- model contains a parameter (or parameters) that explicitly control trait fitness
- metric be injective wrt to model parameters (domain) and metric (codomain)
- chosen metric be monotonic wrt to fitness parameter
- standardisation

The first because we need to be able to intervene (in a causal sense) on fitness.
The second because we must be able to map back from the metric to a unique set of model parameter values.
The third because the function (metric) should monotonically increase as trait fitness increases
The fourth so that we can compare different ``evolutionary'' scenarios using a common framework.

(Note that if I standardise the metric so that s = -1 is a metric of -1, then it almost certainly won't be injective because there will be multiple fixation probabilities of 0...for this reason, I'd need to scale from the highest selection coefficient giving zero fixation. I think I'll just abandon trying to get an analytical expression for the metric and instead just present it in algorithmic form (with the revised method, I'd need to recalculate the metric for each evolutionary scenario, so there wouldn't be much point in doing it analytically since that would fail for anything but the most simplest cases))

A candidate that fulfils these criteria is fixation probability, or as we will implement it, persistence probability (i.e. 1 - probability-allele-of-interest-is-lost). (Actually this will be more like 1 - probability-allele-of-interest-is-not-within-specified-frequency-range in order to account for the case of polymorphisms where we might want to ask a question like ``why is allele x maintained at 30\% +/- 5\%?''. So maybe persistence probability isn't the best phrase...

Desired characteristics:
- metric can be calculated from an output that is categorical (for information theory purposes)
- can account for a trait's evolution as well as its maintenance
- can account for a trait going extinct more slowly than another (e.g. Paul's argument)
- can account for anything from a single comparison in a simple model (e.g. haploid fixation probability) to a complex selection regime (function of a trait that invaded an ancestral population and has persisted in the face of multiple competitors)

Desired (but less important)
- if s = 1 is metric = 1, then a log scale type relationship so that s > say 0.1 have high metric values

Requires estimates of the model parameters, e.g. Ne, s, h (diploid), etc.
Also requires specification of selection regime (effectively an evolutionary history), which includes the period of time.

METHODS: 

(old)-for now, use kimura's diffusion approx to see if I can get an equation for the metric
for everything else, I can just simulate (simulation is the best method for the PID in any case since I need the 0/1 output.

- don't think there's much point in doing an individual based model version. even if i want to show ``how to apply it to a more complicated example'', I don't think it needs to be very complicated

What are some potentially interesting cases to explore by simulation?

- Randomly changing the strength of selection? Here I could show that fixation probability automatically accounts for a variable strength of selection over time. But how is this different to different fitnesses in different environments? I'm not sure it is fundamentally different (I think the analogous case is changing environments) but there might be some differences in terms of counterfactual comparisons. For example, for a given set of parameters in a model that has fluctuating selection, whether one or the other allele is expected to fix can be determined by the expected frequency of environmental changes. If we change the expected frequency of environmental changes, we might change which will fix. This isn't a problem for inference in this case. But in the case of the fluctuating selection coefficient through time, the counterfactual comparison is less obvious. Is it a version of the allele that doesn't have a fluctuating selection coefficient, one that does but fluctuates at a different rate, etc.? I guess it depends on the particular question being asked, but I think a counterfactual would have to be subject to a resident population with the same parameters as its comparison. A more pertinent concern is how decompose the fitness contributions of a changing selection coefficient, as arguably they represent different functions (but more temporally separated). The specific mechanism of a changing selection coefficient matters---I guess it must be a novel environment. And in that case, what we are really doing is just increasing the number of environments, but rather than having multiple environments available at one time, there is a sequential procession of environments.

But this highlights a potential problem with the metric. What if an allele was maintained indefinitely (either at a polymorphism or being fixed)? I only assign metric values based on individual ``competitions''. If something ``wins'' multiple competitions, how do I reflect this? It also implies that I shouldn't be capping function based on an arbitrary selection coefficient of one fixation-probability-competition. Might remove one of my problems, because I could potentially just set s = 1 to metric = 1. Do I then allow function to accumulate additively?

Or do I take a different approach and simply set s = 0 to metric = 0 and s = 1 to metric = 1 over any arbitrary set of conditions? I could then set the metric on a log scale (maybe natural log will work well with Kimura's approximation?) so as not to have everything bunching up around metric = 0 (so then there isn't a linear relationship between s and metric as I had previously stipulated). But do I then lose the simplicity of having the metric defined relative to the simplest Wright-Fisher model? I guess so, but perhaps it isn't a bad thing. It then boils down to a problem of identifying the underlying model parameters (and construction, if we lived in a world where multiple models were possible).
Even if it would be nice to have the analogy with Ne retained, I then can no longer compare models over different regimes (if I have to do an additive metric thingie, then something that is maintained in the population for a long time could have a metric value of 100). Taking this approach makes it very easy to standardise the metric between -1 and 1 for any arbitrary regime (of course it raises a serious concern as to how one would ever infer a generative model and parameter values for that model in the real world but that's always going to be an issue.
- 

Results: 

Discussion and future challenges

- The selected effects theory of function attempts to relate the evolution of traits to biological function.
- assume a simplistic view of evolution, which as a couple of implications:
  + explanation invoking Proper functions might give misleading accounts of evolution (covered in programmatic paper)
  + has a dichotomous view of function---either something is has a function or it doesn't. While the philosophy literature often talks about function or functions, there isn't really a framework for accounting for a trait having multiple functions...or at least this is better formalised as partial functions (i.e. decomposing total function into subfunctions).

  - a single locus trait can have multiple functions, each of which contributes in some way towards its evolution (including interactions between effects)...seems like I'm wanting to deny ``causal parity'' here (at least for a more fine grained causal explanation)
  - strength of selection may vary throughout a trait's evolution (e.g. distinction between evolved for vs maintained)


  Will align the theory of selected effects with biological reality (come up with an example of each)

  current view is implicitly levelling an arbitrary measure of ``this effect is sufficiently causally important'' as to justifying being called a function--but what is this criterion?
  part of this might be the grain at which the explanation takes place---if it is sufficiently coarse grained then you might be able to justify the notion of the function of x is y...but what if you want a finer grained notion?

  Indeed, I think one could make a case for the fact that the very fact we do not have a concept of graded functions forces philosophers to use vague terms such as ``adverts to'', etc.---they simply don't have any other framework with which to use.

The level (or grain) of causation explanation might be a common thread with which to tie the piece. Take causal parity. I think it's fine to talk generally of developmental biology as not endowing genes or environment as having causal precedence; however, for any particular biological case, I think it is absolutely correct to apportion causes in a non-parity manner. So perhaps one of our criticisms of proper function is that it is a very coarse grained approach that creates a caricature of evolution. 

distinction between allele (or whatever you're measuring the trajectory of) as having a function or being selected for and the particular way in which one might decompose that ``total function''
  
Our aim in this study is to connect the concepts explored in philosophy of biology with the well-established quantitative methods of population genetics.

Biological function is a more general notion of adaptation (since the latter normally applies to more complicated trait complexes). This fits with our focus on graded notions (not only a graded notion of function but also function as a graded notion of adaptation). I also want to stay away from adaptation other than to mention it at one end of the continuum of function (because last thing i need to do is irk people in the adaptation field)

\section{Abstract}
\label{sec:abstract}

\section{Introduction}
\label{sec:introduction}

(Will be pitching it at biologists. A philosophical version will be written as a separate paper. So here my arguments are solely based around how I'll sell it to biologists.)

- ENCODE and importance of defining function
- different views but often divided into causal role and selected effects
- much of ENCODE debate was over ENCODE's loose (biological activity) definition of function vs. evolutionary biologist's SE-like view
- but we can't actually operationalise SE and it has a number of problems (dichotomous view of function, breaks down when there are complicated evolutionary dynamics, no obvious way how to operationalise it). Simply detecting selection acting on an element isn't enough because to assign a function we need to know how (and how much) that specific function of a contributes to selection on that trait
- highlight that SE theory does not take realistic biology into account; likewise, there are many misconceptions around the notion of SE peddled by biologists
- goal in this study is to build a SE version of function that can actually be applied to simplified cases, marrying evolutionary biology models with the philosophical concept

In 2012, the ENCODE consortium made headlines with their claim that roughly 80\% of the human genome was functional.
A backlash from evolutionary biologists soon followed who claimed that functional elements made up no more than 10\% (cite grauer etc).
This disagreement fundamentally revolved around the different conceptions of function used by the two parties.
ENCODE used criteria for functional element identification that roughly correspond to what philosophers would call biological activity (what a biological item or trait does or is capable of doing) (wouters2003 and the paper he cites).
Evolutionary biologists, on the other hand, proposed that an ascription of function be reserved for only those elements under selection.
This claim has a philosophical analogue called the selected effects, or etiological, view of function.
In short, a trait's selected effect function (SEF) is the trait's effect that caused it to evolve (or be maintained) by natural selection.
Evolutionary biologists pointed to SEF as justification for why function should be restricted for elements under selection.
There is, however, no way to apply SEF to non-trivial real-world evolutionary scenarios (e.g. we have previously shown that its explanations break down whenever evolutionary feedback is present).
It also deviates from mainstream evolutionary theory in several ways: (i) it conceives of functions as binary concepts (something is either a function or not a function); (ii) it denies the notion of relative fitness, which as we will see, means that it cannot distinguish between selection and drift.
For example, evolutionary biologists have suggested applying SEF to regions of the genome under selection.
There are two problems with applying the SEF account to selection on a genome.
First, to apply SEF one must not only identify a region of the genome under selection, but identify the effect (function) of the genomic region.
Second, evolutionary biologists want to be able to distinguish between regions under strong selection, weak selection, and no (nearly-neutral) selection, but SEF does not distinguish between these three cases.
In this paper, we develop a method for quantifying SEF.

\section{Methods}
\label{sec:methods}

We want our metric for partial biological function to satisfy the following desiderata (required properties):

\begin{enumerate}
\item Generalisability/contextualism (can be applied to multiple evolutionary scenarios)
\item Gradation (accounts for the fact that selective forces can vary in strength)
\item Traits can have multiple commensurable functions (a trait may have multiple effects, each of which partially explain why the trait exists)
\item Metric must be surjective function (each selective force maps onto a single metric value).
\item Metric must reflect selective force over history, not effect of drift
\item Must be able to apply metric to a single realised evolutionary trajectory
\end{enumerate}

Functions can thus be partial in several senses.
First, even if a trait only has one function, the degree to which the function has been selected is a graded notion (other things, such as drift, are involved in the spread of a trait).
Second, a trait may have multiple functions, in which case each will partially contribute to selection on the trait.
Third, selection on a trait may be due, in part, to selection on other traits (e.g. genetic linkage (hitchhiking), different traits sharing genetic elements), in which case a trait's function only forms a part of selection on the trait.
Fourth, selection on the trait might have changed over time, or it might differ between environments (this one isn't really a justification for partial functions but rather for decomposing total function so as to better understand the contributions).

With the basic PID script I have now, I can do 3 (diploid), 1 (obviously), I should be able to do 2 but need to think of a model that allows it (two loci?), and I can do 4 (at least the bit about selection strength changing)

For 2:
Say we have a two locus system, 1 and 2, with alleles A/a and B/b respectively.
That gives us the following genotypes: AB, Ab, aB, ab.
So I can get away with the basic PID script if I stick to a haploid model? But then what are the initialisation conditions and counterfactuals? I could start with AB and then introduce one Ab? This doesn't become easier if it's diploid either (although there I could start with AaBb and assume recombination, but this seems very specific...).
I suppose I could just start with AB and then have four separate scenarios (AB vs Ab, aB, ab). I wonder how the decompositions would compare (I'll have the same four fixation probabilities, only what's the focal case and what's the counterfactual will change...it presumably won't decompose the same since fixation probabilities will be dependent upon the resident type). Could also get epistasis from this model if I wanted to say something about synergistic information. This one is going to be a bit of a mess (would also require that I track two different allele trajectories when it comes to matching realized trajectories to a metric--this should be fine though because it just makes it a bivariate time series. Might leave this one for later though.)



\subsection{General approach}
\label{sec:general-approach}

Our focus in this study is to illustrate a quantitative framework by which one might construct a metric for biological function.
To construct the metric, we use simple a implementation of the haploid Wright-Fisher model.
We intend the metric to represent a notion of biological function as it would appear in an idealised population.
In this sense, our metric for biological function has parallels with another population genetic measure: effective population size.
Effective population size is a metric for the degree of genetic drift that is also standardised by the Wright-Fisher model.
In order to illustrate how our metric might be applied to more complicated models, we extend our simple haploid Wright-Fisher model to an individual-based model that explicitly accounts for multiple environmental states.

\section{Metric}
\label{sec:metric-1}

To derive the metric, we will use a simple Wright-Fisher haploid model.
We highlight two assumptions in the Wright-Fisher haploid model because they are relevant for construction of the metric. Our Wright-Fisher haploid model assumes (i) a single locus with two-alleles; and (ii) a direct map between haplotype and phenotype (such that each haplotype has a single phenotypic ``effect'' accounting for its biological fitness).
In addition, we make two assertions: (i) in a population genetics model, fixation probability is the best proxy of the ``evolutionary success'' of a haplotype; and (ii) in the selected effects account of function, a trait's effect on its fitness is a good measure of the biological function of that trait('s effect).

It follows from the model assumptions and assertions that a metric for biological function should be based around the selection coefficient because, in a haploid Wright-Fisher model, the selection coefficient solely determines fitness differences between traits (recall that there is a perfect mapping of haplotypes->phenotypes->traits).

\subsection{Wright-Fisher model}
\label{sec:wright-fisher-model}

The population contains $N$ haploids, and each haploid carries one of two alleles: a mutant allele $A$ or a wild-type allele $a$.
We can track the number of $A$ (mutant) alleles at time $t$, which is given by the random variable $X_t$ (and since there are only two alleles, the number of $a$ alleles at time $t$ is $N-X_t$). 
The fitness of a haploid with the wild-type allele $a$ is $\omega_a = 1$, while the fitness of a haploid with the mutant allele $A$ is $\omega_A = 1 + s$, where $s$ is the selection coefficient.
We assume that the mutant allele $A$ is introduced in a single copy (i.e. initialisation conditions are $X_0 = 1$).
We do not consider mutation.

For generality, we will not talk of the fixation probability of allele $A$, $\Phi_A(t)$, as the \emph{sensu stricto} fixation of allele $A$ where time ($t$) is taken as $\lim_{t \to \infty}$ but rather as the complement of the fixation of allele $a$, $\Phi_a(t)$, over a finite, defined time period ($\Phi_A(t) = 1 -\Phi_a(t_c)$ where $t_c$ is the census time).
This definition of $\Phi_A(t)$ includes the \emph{both} the probability that the $A$ allele has fixed at time $t$, and the probability that neither the $A$ nor $a$ alleles have fixed at time $t$.
This definition has two advantages: (i) it allows us to formulate the metric over an arbitrary period of time; and (ii) it will allow extension of the metric in the future to diploid cases in which stabilising selection is possible (not relevant in the haploid case but see Discussion for more details).

The probability that there are $i$ copies of allele $A$ at time $t+1$ given that there were $j$ copies of allele $A$ at time $t$ is binomially-distributed according to:

\[
  \Pr \left( X_{t+1} = j | X_t = i  \right) = \binom{N}{j} \left(\frac{ i}{N}\frac{\omega_A}{\overline{W}}\right)^j \left(\left(1 - \frac{i}{N}\right)\frac{\omega_a}{\overline{W}}\right)^{N-j},
\]

where $\overline{W}$ is mean fitness and is given by

\[
  \overline{W} = \frac{i}{N} \omega_A + \left( 1-\frac{i}{N} \right) \omega_a.
\]

Since analytical approximations for fixation probability are only accurate in certain limits, in practice we simulate the fixation probability $\Phi_a(t)$ using $t=100N$ (i.e. allowing sufficient time to elapse such that the $A$ allele should have either fixed or been lost) (see Fig. ?? for a comparison with Kimura's diffusion approximation).

\subsection{Individual-based model}
\label{sec:indiv-based-model}

We 

 % While these models are sufficiently simple for us to use as aids in constructing a metric for biological function, they are nevertheless sufficiently complex as to introduce a number of conceptual challenges that must be solved in the construction of a metric (especially in the diploid case).
% - something about how function can be applied to a wide variety of biological complexes, so we want an approach that can scale well
% This approach can thus not only capture the notion of selection acting to maintain a polymorphism over long time scales, but it also accounts for the differential rate at which alleles are lost from the population (important for a the notion of partial function).
% Importantly, we make no distinction between alleles that remain in the population at a non-zero proportion at $t_c$ (something about how this is actually a strength because we can't know the fate of these alleles but as t increases fixA will approach 1-fixa)
% We centre our framework around fixation probability, one of the most important population genetics summary statistics.
% Since we assume perfect additivity when constructing the model, we will not see stabilising selection (i.e a stable polymorphism) due to heterozygote advantage (overdominance) in the diploid model.
% Nevertheless, when \emph{applying} the metric, we will encounter scenarios with stabilising selection.
% This leads to obvious problems for a metric based around the fixation probability of an allele because the fixation probability will not capture those alleles that are selectively maintained in a polymorphism over reasonably finite timescales.
% \begin{itemize}
% \item In population genetics, 
% \item In  Biological function is captured 
% \end{itemize}
% Under our assumptions of constant fitness effects, the selection coefficient is a 
% Our general approach is as follows:
% \begin{itemize}
% \item Create a mapping from simple Wright-Fisher population genetics 
% \end{itemize}
% To that end, we make a number 

\section{Results}
\label{sec:results}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
