* general thoughts

We want our metric for partial biological function to satisfy the following desiderata (required properties):

1. Metric must reflect selective force over history, not effect of drift
2. Metric must be graded/continuous (accounts for the fact that selective forces can vary in strength)
3. Traits can have multiple commensurable functions (a trait may have multiple effects, each of which partially explain why the trait exists)
4. Metric must be a increasing monotonic function of increasing selective force
5. Metric must be surjective function (each selective force maps onto a single metric value).
6. Generalisability/contextualism (can be applied to multiple evolutionary scenarios)
7. Must be able to apply metric to a single realised evolutionary trajectory

The first because we must be able to distinguish between selection/drift (selected for/selected of, etc.).
The second and third because they reflect the reality of selection by natural selection.
The fourth because biological function is meant to be proportional to the degree to which that effect of the trait was involved in selection.
The fifth because we must be able to map back from the metric to a unique set of model parameter values.
The sixth because we want to be able to apply the metric to a range of theoretical scenarios in which we might ask questions about biological function.
The seventh because, in order to apply the metric to any /actual/ case of evolution, we only have a single realized trajectory to work with.

Functions can thus be partial in several senses.
First, even if a trait only has one function, the degree to which the function has been selected is a graded notion (other things, such as drift, are involved in the spread of a trait).
Second, a trait may have multiple functions, in which case each will partially contribute to selection on the trait.
Third, selection on a trait may be due, in part, to selection on other traits (e.g. genetic linkage (hitchhiking), different traits sharing genetic elements), in which case a trait's function only forms a part of selection on the trait.
Fourth, selection on the trait might have changed over time, or it might differ between environments (this one isn't really a justification for partial functions but rather for decomposing total function so as to better understand the contributions).

Desired characteristics:
- can account for a trait's evolution as well as its maintenance
- can account for a trait going extinct more slowly than another

A candidate that fulfils these criteria is fixation probability, or as we will implement it, a modified form that is better described as a probability of being within a specified range  (i.e. 1 - probability-allele-of-interest-is-not-within-specified-frequency-range; this is in order to account for the case of polymorphisms where we might want to ask a question like "why is allele x maintained at 30% +/- 5%?").

Some decisions that will need to be made:
- How to scale metric? Metric = 0 should be drift; Metric = -1 should be fix_prob of ~0; Metric = +1 should be "paradigmatic" function (this latter one will be the most arbitrary---I could choose selection coefficient of 1, not_lost_prob of 1, or just choose a value based off of the mutational distribution of fitness effects; for all but the latter, I'd probably have to log transform in order to avoid having tiny metric values for everything)
- Balance between a selective expectation (needed to disentangle realized from expected fitness) and explanation of /actual/ evolutionary history (which requires explicit specification of explanatory depth and what to marginalise over). Pure propensity interpretations go too far towards function being an expectation over all possible trajectories that life could have taken (even though the vast majority will not occur). Pure selected effects interpretations would require a metaphysical specification of all possible environmental states that actually occurred, which is obviously infeasible. My approach requires a balance of the two (set the explanatory depth (e.g. grain of description of the environments to consider), record the observed environmental states, and calculate the expected fitness over those realised environmental states.
- Will require specification of selection regime (effectively an evolutionary history), which includes the explanatory depth (e.g. description of environments and observed environmental states), as well as the period of time to consider

What are some potentially interesting cases that I might want to use the metric to compare?
- Single locus, haploid (two allele), each with one effect---simplest setup
- Single locus, diploid (three genotypes), each with one effect, that share an underlying allele (e.g. sickle cell)
- Single locus, haploid (two allele), two different environments. Two sources (alleles), each with four different outcomes, and a single target with two outcomes.
  + Single locus, haploid (two allele), two different (but temporally overlapping) environments (with each allele having an effect in each environment---e.g. bet-hedging). Not sure how this will go if I have to specify the particular environmental states (i.e. stop short of propensity). Each allele will  (A consideration here is whether the effect in each environment should count as a separate function, as arguably a trait might be performing the same action, it just has different effects in the two environments.)
  + Single locus, haploid (two allele) two different (and temporally separated) environments (with each allele having an effect in each environment---e.g. exaptation/vestigial). (A consideration here is whether the effect in each environment should count as a separate function, as arguably a trait might be performing the same action, it just has different effects over the evolutionary history.)
- Single locus, haploid, two effects (e.g. the fitness of allele /a/ is $1 + s_{a1} + s_{a2}$). Two sources, each with four outcomes, and a single target with two outcomes.

I had initially set up the metric for a single fixation event, but a potential problem quickly arose. 
What if an allele was maintained indefinitely (either at a polymorphism or being fixed)? 
I only assign metric values based on individual ``competitions''.
If something ``wins'' multiple competitions, how do I reflect this? Do I then allow function values of individual events to accumulate additively?

Instead, I've decided to take a different approach and simply set the metric relative to a investigator-determined set of conditions.
I then lose the simplicity of having the metric defined relative to the simplest Wright-Fisher model, which would have been nice because of the analogy with effective population size.
Although I originally liked the analogy with $N_e$, I then can no longer compare models over different regimes (if I have to do an additive metric, then something that is maintained in the population for a long time could have an arbitrarily high metric value).
Taking this approach makes it very easy to standardise the metric between -1 and 1 for any arbitrary regime (of course it raises a serious concern as to how one would ever infer a generative model/regime and parameter values for that model in the real world but that's always going to be an issue).

I suppose it would be possible to set it up for comparison with the simple Wright-Fisher model---valid for a single sweep---but this isn't ideal because I'd need to infer both the selection coefficient and the population size.
This differs from effective population size, which assumes neutrality and only has to estimate the population size.
I think this weakens the analogy because in the function case one might have different combinations of population size and selection coefficients that give similar patterns (i.e. a non-unique mapping).
Not an issue for the approach that I'll take---as this is simply inferring the parameters of a specified model---but more of an issue with the model is unspecified, which is what would be the case for the effective population size analogy.

** 20200601 thoughts
   - Instead of log-scaling, I might consider a sigmoid function, as it has a lot of desirable properties (in particular it is bounded between 0 and 1 and it is more sensitive to values around a function value of 0).
     I'll have to decide whether I want the function value to be unbounded above and below (as occurs with the sigmoid function) or whether I should truncate the function (e.g. stretch it horizontally and vertically such that it crosses fix = 1 when function = 1 and fix = 0 when function = -1. In both caess, I'll be comparing all scenarios with a standard reference point, which isn't really desirable (e.g. the longer the time frame we consider, the lower the function value). Perhaps the best option is to use a stretched sigmoid that is bounded at a selection coefficient of 1 (this is, unfortunately, arbitrary). I could also bound the bottom with a selection coefficient of -1. This is nice for symmetry purposes and in fact for the simplest case would simply reduce the function metric to the selection coefficient (but this gets a little messier for some of the other scenarios, e.g. for haploid two effects it would need to be s1 + s2 = -1...this is fine here but it would be problematic if you allowed non-linear interactions, such as with epistasis, so I suspect this isn't very generalisable.).
EDIT 20200618: some of what is written above is confused and incorrect. The longer the time frame we consider, the higher the function value it turns out (I might have been confusing absolute persistence probability with persistence probability relative to a drift scenario. Bounding the bottom with a selection coefficient of -1 wouldn't work because it would violate the injective, non-surjective requirement.
   - For the haploid two environment model, the selection coefficient of the allele matched to the  /first/ environment matters (overwhelmingly) the most. This makes total sense, as a mutant is most liable to be lost in the first few generations (it doesn't matter if it has a larger fitness advantage later on if it goes extinct before then). This, however, is the opposite of what selected effect theorists believe---to them, the recent history is the most important factor. The problem is related to what one means when they say "the evolution of a trait". In my framework here, this means the probability that a mutant (instantiation of a trait) remains in the population. But when a selected effect theorist refers to a "trait", they are slapping a vague label on a phenotype that roughly corresponds to what they are interested in. For example, I can trace a unique path through evolution leading to the evolution of a (specific lineage of a) human heart. But there could have many evolutionary paths that might have led to an organ that philosophers would call a "heart". A way to draw the distinction is if I were to consider a scenario in which there are multiple recurrent mutations from "non-heart" to "heart"---even if most go extinct, if there are enough recurrent mutations and the advantage of "heart" is sufficient, then I can construct a scenario in which there is a probability of 1 that a heart will evolve. SE theorists, I think, implicitly include some of these non-adaptive effects in their ascription of function. For example, consider developing antibiotic resistence in bacteria. Given the huge population sizes of bacteria, we can be quite sure that some mutuation(s) will lead to antibiotic resistence, even though the vast majority of mutations that might lead to antibiotic resistence will go extinct due to drift. If our trait is "antibiotic resistence" then we don't care about the actual genotype underlying the trait (and there could be a HUGE number of unique changes that would map to vague phenotype "antibiotic resistence"). So if a SE theorist wants to tell a story about antibiotic resistence, then they need to acknowledge that they are conditioning on the outcome and then retrofitting a story about why that outcome was so likely. The problem here is that even if the meta-outcome (antibiotic resistence) is almost guaranteed given enough time, the specific outcome (specific genotype to a specific phenotype that is a member of set "antibiotic resistence") is far from guaranteed. This then creates a whole host of problems for SE theorists because if they just want to condition on an outcome, then they need to adopt a propensity point of view, something they avowedly reject. But I think I can still justify my results regardless. For example, I think it is justified to say that feathers might never have be used in flying if they had never had an advantage wrt insulation (and that they might still exist solely for insulation if evolution had take a different path). As to how to deal with this in the paper, I think I just ignore it quantitatively, and just point this out in the discussion (and point out that it's future work that SE theorists need to consider). Note that this also relates a bit to the problem of a trait changing over time (e.g. the human heart now is vastly different from the first heart in the last common ancestor of all hearted organisms). 

If mutation is rare, I think it is justified to say that function in the distant history should be more heavily weighted.
If mutation is common and/or the trait designation is sufficiently general such that the genotype -> phenotype mapping is vast, I don't really think that this should affect our function ascription. I think it is conflating adaptive explanations for why /this particular trait evolved/ with other factors relating to the nature of the question and the probability that the trait will arise in the population.

Note that another way of describing my modelling assumptions is that a mutation leading to a function is vanishingly rare (such that you only get one shot). If mutation is common and/or there are many genotype to phenotype mappings, then we have a different scenario more akin to antibiotic resistence. I think a strong case here can be made that SE theorists conflat adaptive explanations (advantage of a instantiation of a trait) and non-adaptive explanations (population sizes, mutation rate, genotype to phenotype mappings, etc.). If they want to say that recent history should be privileged, then basically I think they are conditioning on a trait already being present at a non-trivial frequency in the population, which is asking a different question than I am asking here (i.e. instead of starting with a single organism with the new trait, they are conditioning on those trajectories that have made it to a proportion >x). I think the only way to deal with this is to point it out and to highlight how a function ascription is relavitised to the scenario being explored (e.g. for SE theorists they might be conditioning on a trait that has made it to >x proportion...this again creates problems with the fact that it just obscures very important information about the evolutionary trajectory (for example, what if the trait was deleterious and you've just conditioned on an incredibly lucky instance of that?)).

I should point out that there's nothing wrong with an approach that considers recurrent mutation in the evolution of a trait. I do that all the time in my models. But again it explicitly requires a propensity approach (i.e. we don't say that we're considering the exact evolution history, like what SE theorists want to say, but rather we say we're considering a general model of the history, which is true in spirit but not in details). Incidentally, I don't think this is going to solve the problem with recent history being less important though. I'd expect that the selection coefficient of hte first environment will always be much more important even with recurrent mutation. So there are a few different issues here that need to be clearly described (opposite inference wrt history, trait changing composition over time, factors other than selection on single trait mutation being important, explicating question and scenario to be examined (e.g. how SE theorists implicitly condition on traits that have already spread, etc.)

I think the desire that SE theorists have to prioritise recent function has reasonable motivation. If we consider a realistic trait, then the underlying genotype (and specific phenotype) is constantly changing over evolutionary time. In this scenario, it does make some sense to priviledge the more recent past because changes in the recent past are more closely aligned with recent changes in the trait's phenotype. E.g. bird feathers---when we ask what is the function of bird feathers, we (implicitly) are asking, at least in part, why are bird feathers the way they are in modern birds? Well a good portion of that answer will come from recent evolutionary changes in bird feathers due to selection on properties related to flight. But this is never explicated and would require an extension of the quantitative theory that I'll outline in this paper.

I think that there is also a connection between conditioning on a trait that has reached a proportion of >x and recurrent mutation/other non-adaptive factors influencing probability that trait spreads. In both cases, it seems almost inevitable that a trait will evolve (and that this evolution will be influenced by selection). In both cases, there's also some important non-adaptive dynamics that are omitted. This is a separate issue from the one above, however, and frankly I'd rather not address it if possible (it's messy and doesn't have as clean a take-home message as the one above).

EDIT: 20200618. One nice way to make this point is to highlight that my approach is designed to get at the "explains why the trait exists" concept, and leverages this to build a metric. It's going to be hard for SE theorists to deny the validity of the metric framework if so doing means they have to abandon this concept (given its the central appeal of SE function). If they want to condition on a trait already existing and then ask more specific questions about why it is the way it is, then they're no longer asking why it exists but rather why is it the way it is given that it already exists. I think this is a pretty important point. If we want to say that the function of feathers is for flight, that's totally fine. There has been plenty of selection for morphological features based on advantages associated with flight. But that's still a claim for why feathers exist /in a particular morphological state/ not why feathers exist at all. Of course, the metric approach is very flexible and one might ask a question about why feathers exist in a particular morphological state (as opposed to a different morphological state) and get a metric for that, but this is not explaining why feathers exist.
* 20200618 thoughts
** inverse problem
Without having attempted it, it seems quite obvious that it will not be possible to solve the inverse problem to disentangle the two selection coefficients in the haploid_two_effects model. This will suffer from what is essentially the multicollinearity problem with regression coefficients in stats. The only thing that will matter for inference will be the sum of the two selection coefficients associated with allele/trait A. They should show identical statistical properties. So if the fitness of allele A was 0.7 there would be no way to disentangle 0.5 + 0.2 from 0.3 + 0.4, etc. I suspect I'll be able to predict the metric just as well but I think the results relating to relative effects of each component of allele A fitness will be nonsense.

The two environment case will also be a bit messy. This isn't a "simple" parameter value inference because there are no transition probabilities between environments the way I need to set it up (in order to satisfy SE theorists desire to capture "actual" evolution). Will need some way of representing the number of generations in each environment and of capturing the interaction between environment and the selection coefficients. (Because of the interaction with a specified environment, the selection coefficients are essentially changing over the course of the simulation rather than being fixed parameters.)
** comparing philosophical intuitions with metric results
Considering developing a questionnaire and sending it around to a handful of philosphers that are active in this area. One thing I want to do is compare some implications from the metric wrt certain philosphical intuitions with the reasons that philosophers give for those intuitions.

For example, take the distinction between adaptation and adapted. There is a time component here in that if a trait's effect has only had a positive effect in the current environment for a short while then we don't want to assign it function status. But at a certain point, it moves from adapted to an adaptation. What guides that intuition? The reason this is interesting is because the metric captures a similar quantitative relationship if I shift the time frame over which I calculate the metric (i.e. it seems to monotonically increase). However, this increase concerns the actual spread of the trait (rather than some period of time that the trait has been fixed or prevalent). Writing this makes me realise that actually simply following the spread of a single mutant is perhaps not the best comparison here with the philosophical intuition.

** back mutations and intuition about our confidence in a trait's function increasing over time
What might be better is to do something like allow multiple back mutations whereby a single copy of the trait (assuming it exists) mutates back to the resident type. If we repeat this a number of times, it will act to increase the function value over time, as the counterfactual comparisons in which neutral traits evolved now have a greater chance of being invaded. This might be a nice way to show how the metric is flexible enough to be applied to cases in which a trait is fixed (by essentially comparing the probability of being invaded and replaced rather than probability of invading).

Note that I'll have to be a bit careful here as it's important for the specified regimes to be consistent. Doing something like introducing a back mutation at generation 134 is going to lead to divergences in regimes because the population state at generation 134 might differ between simulations in which the trait is still present (e.g. the back mutation might represent a different proportion of the trait if the trait's frequency differs between simulations). Perhaps the best way to address this would be something like a separation of timescales (like is assumed by adaptive dynamics, etc.) by which I actually specify that a particular equilibrium or absorbing state be reached before introducing another mutant. Since the models are stochastic simulations, an equilibrium isn't going to work (and besides it would only apply to the diploid model). Better would be restricting this to the haploid single environment model and making it contigent upon the trait having fixed (which is an absorbing state). That way, there's an easily identifiable path for any trait that persists (basically has reached fixation and resisted every potential invader). This way, I could specify a consistent regime that was defined something like "10 invaders with selection coefficients x1, x2, x3...x10 introduced sequentially whenever the trait reaches fixation". The final comparisons are then probabilities of persistence of the focal trait compared to the probabilities of persistence of the counterfactual traits (i.e. ones that have a selection coefficient of zero). The more invaders a trait is subjected to, the higher its function will become (as the neutral counterfactual traits are more likely to be invaded and go extinct, so the joint probability that fix originally and withstand all the invaders will decrease more quickly than with the focal trait). I think this actually captures the intuition behind why a trait's function should increase (or our confidence in that effect being a function of the trait) should increase over time, but it explicatese it quite nicely.

As an added bonus, I can at the same time show why we can consider a trait that is fixed as having a function (because its probability of persisting in the face of invaders is higher than a counterfactual trait).

EDIT: see 20200619 below for some thoughts on how this relates to the evolved for/maintained by distinction
* 202006019
** evolved for/maintained by distinction
I think a similar line of thought to that from yesterday (on the adapted/adaptation distinction) applies to the evolved for/maintained by distinction. One reason that we might want to preference more recent function is because as time passes (and the origin part of the equation becomes more distant), we want to assign a role of selection to the active maintenance of a trait in a population. This seems justified--in a realistic scenario, a trait that is fixed will be invaded by mutants, each of which itself has a chance to become fixed and thereby causing our trait of interest to no longer persist. The longer the maintenance phase, the more of these challenges that would occur, and the more strongly we could justify highlighting the maintenance aspect of the trait's function. If I alter the haploid two environment model to allow for an origin phase (in which the trait has its first effect and either fixes or goes extinct) and a maintenance phase (in which the trait has its second effect and is invaded by multiple mutants), I think this will be reflected in the model results. The origin portion will have some fixed probability but as we extend the maintenance portion (e.g. by considering a longer time period and more invading mutants), the joint probability that we still observe our trait will become relatively more weighted by the maintenance effect. Note that if I implement this, then I am no longer considering a fixed number of generations for each type but rather separate regimes in which each one ends with the trait becoming fixed (conditional on the trait not going extinct at any point).

I think this will be very illustrative. We can apply it to the exaptation case, in which a trait that was previously neutral during the origin phase finds itself as having an advantage during the maintenance phase. At the very beginning of the maintenance phase, it will have a function metric equal to zero, as it is at that point indistinguishable from the countefactual drift scenario. Over time, however, its function value will increase because it will have a higher probability of resisting invasion by new mutants compared to the counterfactual drift scenario. This agrees very nicely with intuition about this case.

We can likewise consider mismatch as a case in which a trait had an advantage during the origin phase but now in the maintenance phase finds itself without a fitness advantage (or a disadvantage or less of an advantage, etc.). When the mismatch first occurs, its function value is not diminished in any way (as at this point it is indistinguishable from a case that will persist under the same selective conditions as occurred during the origin phase), but as the mismatch persists, its function value might decrease^ because the trait's mismatched effect reduces the probability that the trait is maintained in the face of invading mutants.

^ Note that its function value could actually increase (if in the maintenance phase the trait still has an advantage over drift, albeit a diminished advantage compared to the origin phase) but it would decrease relative to what it would have been had its original advantage been maintained (because the function metric is relative to the neutral counterfactual).

I should make sure that I have a good think about all the changes I want to make to the C++ code so I can implement everything in one go.
* 20200701
Will briefly meet with Paul to discuss a couple of things before he goes on break.

- quick discussion on separating out non-selection "events" as a time-series of "actual" outcomes rather than transition probabilities/expectations. This applies not only to environments but also to mutations. Specifically I'm interested in whether this accurately puts into practice comments in Paul's 1992/93 papers (they are sort of phrased in terms of expectations but I suspect that Paul is thinking of realised fitness. I just want to confirm this and that my implementation will be acceptable to the more straight philosopher crowd.)
- I want to demonstrate that an SE explanation is relativitised to a particular specification and not an absolute explanation in any sense (this is largely because I want to communicate the point that SE requires expectation in order for it to be tractable; without tractibility, you lose the compact explanation, which is the key appeal of proper functions). I should be able to demonstrate this by moving from the haploid single environment to the haploid two environment scenario. 

The idea here would be to set up a scenario in which fitness across two environments can be marginalised to give the same expectation. That is to say, whether I actually divide up the environment into two "patches" (i.e. separate out the joint probabilities) or I don't (i.e. I marginalise over the environments and consider a homogeneous environment), it doesn't affect the expected outcome. But given that we calculate the function over the actual environmental states, we'll get a different function value in these two cases. This implies that realised fitness---but not expected fitness---depends on the grain of description. In turn, this implies that biological function also depends on the grain of description. By not defining a finer level of description, we miss part of the explanation (more importantly, we would calculate a different function value).

I can then show that this seemingly implies that the haploid single environment is not a valid description because our estimation of the function value can always be refined (and altered) by a finer description
even if a trait's expected fitness is not altered by subdividing environments, etc (this gives a proof for $n=1$). By induction, we could apply this process from any $n=k$ to $n=k+1$, thereby also invalidating the haploid two environment model, the three environment model, and so on. We'd only have an exhaustive description once we have an exhaustive subdivision of all the possible environments, which is not only intractable but defeats the purpose of proper function providing a compact description of why a trait exists (that reflects the actual evolutionary story).

While this obviously applies to the expected vs actual distinction, SE theorists might just hold this up as an illustration of why using expectation is misguided (or perhaps even why trying to develop a quantitative theory of biological function is misguided).
I need to convey that my point is more general than this. Fitness is always an expectation for a given set of conditions that we (marginalise over and) lump under a label that reflects the marginalisation ("hot environment" for example).
By subdividing the environment, in a sense we unwind some of the marginalisation and utilise separate joint probabilities (e.g. subdividing hot environment into warm and very hot environments).
So long as this subdivision does not alter the expectation of a random variable, can always take a category that we marginalise over, subdivide the expectations, and relabel the subdivisions, thereby providing a finer grain of description.
We have changed nothing other than the grain of description; the evolutionary expectation remains the same.
But if we then measure the /actual/ outcome of this fine-grained variable and apply the metric, we will end up with a different answer because we've converted part of the probability distribution from an expectation to a realised outcome (so in a way we've discretised the distribution into taking one of the available options at each time point).
So the biological function value depends on the grain of description if one insists upon taking actual events rather than expectations.

Note that not only can can one subdivide a particular grainness of description by increasing the number of categories (e.g. hot, cold to hot, medium, cold and so on), but also by adding other variables (e.g. precipitation, foliage cover, etc.). Ultimately a broad category is just the joint probability of all the finer features that lead to a specific outcome (e.g. hot temperature is a manifestation of a huge number of environmental conditions, some of which will lead to rain some of which won't...we marginalise over the precipitation outcomes if we don't care about it but we can also just subdivide the probability space by specifically adding precipatation into our joint probabilities e.g. instead of just P(hot) we can divide it into P(hot, rain) and P(hot, ~rain) where P(rain) + P(~rain) = 1.

What are the conditions necessary for this to hold? One is obviously if precipitation has no effect whatsoever on fitness, but this isn't interesting. One relevant case is where precipitation does affect fitness but in which the distribution (rain, ~rain) is the same in hot environments as in cold environments (i.e. (rain | hot) = (rain | cold)). Another is where precipitation does affect fitness, and the distribution of precipitation is different in hot and cold environments, but in which we simply attribute some of the effect of precipitation on fitness to temperature (e.g. if fitness were higher when temp is hot (controlling for precipitation) and when precipitation is ~rain (controlling for temperature), then if we only considered temperature we'd attribute a higher fitness to hot environments if there's a correlation with hotness and ~rain. In this case, by adding in the environmental variable for precipitation, we'd have a more complete causal story because we could separate fitness into (fitness | hot, rain), (fitness | hot, ~rain), (fitness | cold, rain), (fitness | cold, ~rain) rather than just (fitness | hot), (fitness | cold). 

If we were initially only measuring temperature, then we could only obtain actual readings of hot or cold at each generation and therefore only calculate (fitness | hot) or (fitness | cold).
Our realised fitnesses (due to the actual temperature) will therefore differ from our expected fitnesses (given by expectations calculated from transition probabilities between the different environments).
Biological function using actual environmental measures therefore differs from what it would be using expectations.
Measuring precipitation just moves this one step up the ladder.
Now if it's hot, we can distinguish between (fitness | hot, rain) or (fitness | hot, ~rain), each of which has a different value.
Like our measure of temperature at each generation, we now have a measure of precipitation at each generation that will differ from the expectation.
It's this latter point that is key---when we only had (fitness | hot), we marginalised over precipitation, which means we took the expectation due to precipitation.
(If we calculate our fitness expectation based on 70% expectation of being ~rain and 30% of rain, then we'll use these values regardless of what % of days it actually rains.)
When we actually measure and account for precipitation, however, we'll apply it to the actual number of days that it rains.

One possible interpretation is that as we take a more fine-grained approach (i.e. incorporate more variables), we actually causally account for some of what we would have called "drift" (e.g. an individual that had more than expected offspring in the "hot" environment might have done so in part because it was hot and because there was rain (assuming that rain increases fitness). Without accounting for precipitation, we'd just attribute this to a random deviation from expected fitness in hot environments.

This gets at a fairly thorny issue though, namely where we get our expectations from, which leads into the philosophy of probability (which I want to mostly avoid).
It is important though to distinguish between expectation due to a priori reasoning of some sort (e.g. a coin has a 0.5:0.5 chance of h:t because it's perfectly weighted) vs expectation due to observation (a coin has a 0.5:0.5 chance of h:t because as the number of flips approaches infinity we converge on this mean).
I bring this up because arguably the expected fitness of an organism in an environment would be experimentally derived from how it would do if we ran a bunch of independent trials in that environment vs alternatives.
That might work for experiments (although in contrived setups with specific, fixed conditions) but in practice we can't do it for actually observing evolution because at best we only have a small number of pseudo independent trials (outcomes of evolution at each generation in a lineage).
Ultimately it doesn't matter for this project because I'm all-knowing and can just state expectations a priori. It will, however, affect the correspondence between our models and "reality".
This is probably interesting in the context of using machine learning to solve the "inverse problem" by inferring actual environmental states over time (rather than transition probabilities between environments).
Maybe this last bit is all rambling nonsense that won't make sense to me later, but it does seem like there's something unsettling about having to, at the same time, infer expected fitness (selection coefficients) and actual environmental states at each generation (which combined with selection coefficients give the realised fitness). I think I might be able to do it in the examples I use here (limited interactions, can make simplifying assumptions about trait-environment interactions), but seems like it would be a mess in a more complex example (I think my worry here is that a mismatch between the model and "reality" combined with ambiguities with inferring selection coefficients and actual environments could easily lead to parameter inference that is consistent with the data but quite far from what actually occurred). Then again, this might simply be a methodological consideration, so I need to wait before I pass judgement on that.

Finally a thought that if I do go through this in either the biology or philosophy paper with John, then I should use a coin flip example to get across the intuitive point. For example, the environmental property "wind" might not affect the coin flip overall, but maybe a wind blowing at 5 m/s from the NW makes heads more likely. If we aren't measuring wind, then we just view this as random deviation from expectation (heads with probability 1 instead of 0.5:0.5), but if we are measuring wind, then our expectation of heads has increased by our knowledge of the wind speed/direction (and so we view it with a smaller degree of randomness than when we don't account for wind).

* 20200702
meeting with Paul
- one way to contrast the difference between an explanation based around uncertainty (e.g. transition probabilities between environments) vs one around "actual" events, such as that desired by SE theorsits, is program vs process / comparative vs contrastive. In Sex and Death, Paul and Kim contrast "actual sequence explanations" with "robust process explanations" (pg 84).
- also Paul pointed out that Wesely Salmon has some philosophy of probability that covers some of these aspects (read salman1979.pdf)
* resources
https://towardsdatascience.com/deep-learning-for-time-series-classification-inceptiontime-245703f422db
fawaz2019a and fawaz2019b
lines2016
https://arxiv.org/pdf/1909.04939.pdf
https://github.com/twosigma/flint
https://www.deeplearningbook.org/
https://learning.oreilly.com/library/view/deep-learning-with/9781617294433/OEBPS/Text/title.xhtml
https://towardsdatascience.com/how-to-train-your-neural-networks-in-parallel-with-keras-and-apache-spark-ea8a3f48cae6
